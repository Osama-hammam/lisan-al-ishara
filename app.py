import os
from flask import Flask, render_template
from flask_socketio import SocketIO, emit
import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model
import time
from collections import deque, Counter
import base64
import io
from PIL import Image, ImageFont, ImageDraw
import threading
import tempfile
import requests
import pygame
import arabic_reshaper
from bidi.algorithm import get_display

# --- ElevenLabs API Configuration ---
# It's highly recommended to use environment variables for sensitive keys in a real application
API_KEY = 'sk_e883cf78c9b9e06bb7236ca8ba711d0f5fe77281579f616b' # Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ù…ÙØªØ§Ø­ Ø§Ù„Ù€ API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
VOICE_ID = 'wxweiHvoC2r2jFM7mS8b' # Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ù…Ø¹Ø±Ù Ø§Ù„ØµÙˆØª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
# --- End ElevenLabs API Configuration ---

# --- Text-to-Speech Function ---
# Ensure pygame is initialized for mixer
pygame.mixer.init()

def speak_elevenlabs(text):
    def thread_speak():
        try:
            url = f"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}"
            headers = {
                "Accept": "audio/mpeg",
                "Content-Type": "application/json",
                "xi-api-key": API_KEY
            }
            data = {
                "text": text,
                "model_id": "eleven_multilingual_v2",
                "voice_settings": {
                    "stability": 0.5,
                    "similarity_boost": 0.8
                }
            }

            response = requests.post(url, json=data, headers=headers)
            response.raise_for_status()

            with tempfile.NamedTemporaryFile(delete=True, suffix=".mp3") as fp:
                fp.write(response.content)
                fp.flush()
                if not pygame.mixer.get_init():
                    pygame.mixer.init()
                pygame.mixer.music.load(fp.name)
                pygame.mixer.music.play()
                while pygame.mixer.music.get_busy():
                    time.sleep(0.1)

        except Exception as e:
            print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… ElevenLabs: {e}")

    # Start speaking in a new thread to not block the main camera stream
    threading.Thread(target=thread_speak).start()
# --- End Text-to-Speech Function ---

# Ø¥Ø¹Ø¯Ø§Ø¯ Flask Ùˆ SocketIO
app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key' # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
socketio = SocketIO(app, cors_allowed_origins="*")

# Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù…Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
interpreter = None
hands_detector = None
camera_thread = None
camera_running = False
camera_cap = None
CAMERA_INDEX = 0  # Ø±Ù‚Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: 0 Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©ØŒ 1 Ù„Ù„Ø«Ø§Ù†ÙŠØ© ÙˆÙ‡ÙƒØ°Ø§

# --- Drawing Functions ---
def draw_styled_landmarks(image, results):
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(
                image, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),
                mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)
            )

def draw_confidence_bar(image, confidence):
    bar_width = 200
    bar_height = 20
    fill_width = int(bar_width * confidence)
    cv2.rectangle(image, (10, 60), (10 + bar_width, 60 + bar_height), (255, 255, 255), 1)
    cv2.rectangle(image, (10, 60), (10 + fill_width, 60 + bar_height), (0, 255, 0), -1)
    cv2.putText(image, f'Confidence: {confidence:.2f}', (10, 55),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)

def draw_arabic_text(image, text, position, font_size=36):
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    
    font_path = os.path.join(os.path.dirname(__file__), "NotoNaskhArabic-Regular.ttf")
    if not os.path.exists(font_path):
        print(f"ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø· {font_path}. Ù‚Ø¯ Ù„Ø§ ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.")
        font = ImageFont.load_default() # Fallback to a default font
    else:
        font = ImageFont.truetype(font_path, font_size)

    img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)

    # Calculate text size using getbbox for more accurate positioning
    bbox = draw.textbbox((0, 0), bidi_text, font=font)
    text_width = bbox[2] - bbox[0]
    # text_height = bbox[3] - bbox[1] # Keep for reference if needed

    # The test code had sentence at x=10, then adjusted based on image width.
    # We will position it 10px from the right edge for RTL text
    x_pos = image.shape[1] - text_width - 10
    y_pos = position[1] # Use the provided y-position

    draw.text((x_pos, y_pos), bidi_text, font=font, fill=(255, 255, 255))
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# --- End Drawing Functions ---

class SignLanguageInterpreter:
    def __init__(self, model_path):
        try:
            self.model = load_model(f'{model_path}.h5')
            print(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_path}.h5")
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise e
        
        try:
            with open(f'{model_path}_actions.txt', 'r', encoding='utf-8') as f:
                self.actions = [line.strip() for line in f if line.strip()]
            print(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.actions)} Ø¥Ø´Ø§Ø±Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ù")
        except FileNotFoundError:
            print(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù: {model_path}_actions.txt")
            raise FileNotFoundError(f"Ù…Ù„Ù Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {model_path}_actions.txt")
        
        # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Scaler)
        try:
            self.scaler_mean = np.load(f'{model_path}_scaler_mean.npy')
            self.scaler_scale = np.load(f'{model_path}_scaler_scale.npy')
            print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠØ¹")
        except FileNotFoundError:
            print("ØªØ­Ø°ÙŠØ±: Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (scaler) ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù‚Ø¯ ÙŠØ¤Ø«Ø± Ø°Ù„Ùƒ Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
            # Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ·Ø¨ÙŠØ¹
            self.scaler_mean = np.zeros(126) # 21 keypoints * 3 coords * 2 hands = 126
            self.scaler_scale = np.ones(126)
        
        self.no_frames = 30
        self.stability_window = 12
        self.confidence_threshold = 0.85 # Adjusted to match test code
        self.sequence = deque(maxlen=self.no_frames) # ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        self.prediction_buffer = deque(maxlen=self.stability_window) # Ù…Ø®Ø²Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        self.sentence = [] # Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù…Ø©
        self.last_prediction_time = 0
        self.cooldown_period = 2.0 # ÙØªØ±Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø¥Ø¶Ø§ÙØ© Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
        self.visual_flash = 0 # Ù„Ù„Ù…Ø¤Ø«Ø± Ø§Ù„Ø¨ØµØ±ÙŠ Ø¹Ù†Ø¯ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©
    
    def scale_features(self, features):
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        return (features - self.scaler_mean) / self.scaler_scale
    
    def predict(self):
        if len(self.sequence) == self.no_frames:
            try:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© ÙˆØªØ·Ø¨ÙŠØ¹Ù‡Ø§
                scaled_sequence = self.scale_features(np.array(self.sequence))
                # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                res = self.model.predict(np.expand_dims(scaled_sequence, axis=0), verbose=0)[0]
                confidence = np.max(res)
                predicted_class = np.argmax(res)
                return predicted_class, confidence
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
                return None, 0
        return None, 0
    
    def update_state(self, predicted_class, confidence):
        current_time = time.time()
        new_word_added = False
        predicted_word_text = None

        if confidence >= self.confidence_threshold:
            self.prediction_buffer.append(predicted_class)
            
            if len(self.prediction_buffer) == self.stability_window:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
                common = Counter(self.prediction_buffer).most_common(1)[0]
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†Ø³Ø¨Ø© ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙƒØ§ÙÙŠØ© (80% Ù…Ù† Ù†Ø§ÙØ°Ø© Ø§Ù„Ø«Ø¨Ø§Øª)
                if common[1] >= int(0.8 * self.stability_window):
                    final_pred = common[0]
                    if final_pred < len(self.actions):
                        predicted_word_text = self.actions[final_pred]
                    else:
                        predicted_word_text = "Unknown" # Ø­Ø§Ù„Ø© ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø©
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ù…Ø±Øª ÙØªØ±Ø© Ø§Ù„ØªÙ‡Ø¯Ø¦Ø©
                    if (not self.sentence or # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¬Ù…Ù„Ø© ÙØ§Ø±ØºØ©
                        predicted_word_text != self.sentence[-1] or # Ø£Ùˆ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
                        current_time - self.last_prediction_time > self.cooldown_period): # Ø£Ùˆ Ù…Ø±Øª ÙØªØ±Ø© Ø§Ù„ØªÙ‡Ø¯Ø¦Ø©
                        
                        self.last_prediction_time = current_time
                        self.sentence.append(predicted_word_text)
                        self.prediction_buffer.clear() # Ù…Ø³Ø­ Ù…Ø®Ø²Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù„Ø¨Ø¯Ø¡ Ø§ÙƒØªØ´Ø§Ù ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©
                        self.sequence.clear() # Ù…Ø³Ø­ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ù„Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø¥Ø·Ø§Ø±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
                        self.visual_flash = 3 # Ù„ØªØ£Ø«ÙŠØ± Ø¨ØµØ±ÙŠ
                        new_word_added = True
                        print(f"ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©: {predicted_word_text}")
        
        return new_word_added, predicted_word_text
    
    def reset_after_no_hand(self):
        self.sequence.clear()
        self.prediction_buffer.clear()
    
    def get_current_sentence(self, max_length=10):
        # Ø¹Ø±Ø¶ Ø¢Ø®Ø± 10 ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø©
        return ' '.join(self.sentence[-max_length:])
    
    def clear_sentence(self):
        self.sentence.clear()
        print("ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ø¬Ù…Ù„Ø©")

def mediapipe_detection(image, model):
    # Ù‚Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ø£ÙÙ‚ÙŠÙ‹Ø§ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ RGB
    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)
    image.flags.writeable = False # Ù„Ø¬Ø¹Ù„ Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ÙƒØªØ§Ø¨Ø© Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
    results = model.process(image) # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙŠØ¯
    image.flags.writeable = True # Ø¬Ø¹Ù„Ù‡Ø§ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ÙƒØªØ§Ø¨Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
    return cv2.cvtColor(image, cv2.COLOR_RGB2BGR), results # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ BGR

def extract_keypoints(results):
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙŠØ¯ Ø§Ù„ÙŠØ³Ø±Ù‰ ÙˆØ§Ù„ÙŠÙ…Ù†Ù‰
    lh = np.zeros(21*3) # 21 Ù†Ù‚Ø·Ø© * 3 Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª (x,y,z)
    rh = np.zeros(21*3)
    if results.multi_hand_landmarks:
        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
            handedness = results.multi_handedness[idx].classification[0].label
            landmarks = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()
            
            # MediaPipe 'Right' hand is the user's left hand (appears on the right side of the flipped image)
            # MediaPipe 'Left' hand is the user's right hand (appears on the left side of the flipped image)
            # This mapping is crucial to match how the model was trained.
            if handedness == 'Right': # MediaPipe's 'Right' (Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠØ³Ø±Ù‰ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…)
                rh = landmarks # ØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ù…ØªØºÙŠØ± Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠÙ…Ù†Ù‰
            else: # MediaPipe's 'Left' (Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠÙ…Ù†Ù‰ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…)
                lh = landmarks # ØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ù…ØªØºÙŠØ± Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠØ³Ø±Ù‰
    return np.concatenate([lh, rh]) # Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙŠØ¯ÙŠÙ†

# --- ÙˆØ¸ÙŠÙØ© ØªØ´ØºÙŠÙ„ Ø¯ÙÙ‚ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙÙŠ Thread Ù…Ù†ÙØµÙ„ ---
def camera_stream_thread():
    global camera_running, camera_cap, interpreter, hands_detector

    camera_cap = cv2.VideoCapture(CAMERA_INDEX)
    # Ø¶Ø¨Ø· Ø­Ø¬Ù… Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¥Ù„Ù‰ 1 Ù„ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ±
    camera_cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    camera_cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    camera_cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    if not camera_cap.isOpened():
        print("Ø®Ø·Ø£: ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù„Ù„Ø¨Ø«.")
        socketio.emit('camera_error', {'message': 'ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ù‡Ø§ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¢Ø®Ø± ÙˆØ£Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø±Ù‚Ù… 0 ØµØ­ÙŠØ­Ø©.'})
        camera_running = False
        return

    print(f"ØªÙ… ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø±Ù‚Ù… {CAMERA_INDEX} Ù„Ù„Ø¨Ø«.")
    
    last_hand_time = time.time() # Ù„ØªØªØ¨Ø¹ Ø¢Ø®Ø± Ù…Ø±Ø© ØªÙ… ÙÙŠÙ‡Ø§ Ø§ÙƒØªØ´Ø§Ù ÙŠØ¯

    # ØªÙ‡ÙŠØ¦Ø© MediaPipe Hands Ø¯Ø§Ø®Ù„ Ø§Ù„Ø«Ø±ÙŠØ¯
    with mp_hands.Hands(
        max_num_hands=2,
        min_detection_confidence=0.75,
        min_tracking_confidence=0.7
    ) as hands:
        hands_detector = hands # ØªØ¹ÙŠÙŠÙ†Ù‡ Ù„Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ø¹Ø§Ù…

        while camera_running and camera_cap.isOpened():
            success, frame = camera_cap.read()
            if not success:
                print("ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªÙ‚Ø§Ø· Ø¥Ø·Ø§Ø±ØŒ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§...")
                camera_cap.release()
                time.sleep(1) # Ø¥Ø¹Ø·Ø§Ø¡ ÙˆÙ‚Øª Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù„Ù„ØªØ­Ø±ÙŠØ±
                camera_cap = cv2.VideoCapture(CAMERA_INDEX)
                camera_cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                camera_cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                camera_cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                if not camera_cap.isOpened():
                    print("ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
                    socketio.emit('camera_error', {'message': 'ØªØ¹Ø°Ø± Ø¥Ø¹Ø§Ø¯Ø© ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.'})
                    camera_running = False
                continue

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… MediaPipe ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
            image_processed, results = mediapipe_detection(frame, hands_detector)

            keypoints = extract_keypoints(results)
            prediction_text = ""
            confidence = 0
            new_word = False
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ ÙŠØ¯ (Ù†Ù‚Ø§Ø· Ù…ÙØªØ§Ø­ÙŠØ© ØºÙŠØ± ØµÙØ±ÙŠØ©)
            hand_detected = not np.all(keypoints == 0)

            if hand_detected:
                last_hand_time = time.time() # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± ÙŠØ¯ Ù…ÙƒØªØ´ÙØ©
                interpreter.sequence.append(keypoints)
                predicted_class, confidence = interpreter.predict()
                
                if predicted_class is not None:
                    # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙØ³Ø± ÙˆØ¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                    new_word, predicted_text_from_update = interpreter.update_state(predicted_class, confidence)
                    
                    if new_word:
                        prediction_text = predicted_text_from_update
                        # Ø¥Ø°Ø§ ØªÙ… Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ù†Ø·Ù‚Ù‡Ø§ Ù‡Ù†Ø§
                        # Ø­Ø§Ù„ÙŠØ§Ù‹ ÙŠØªÙ… Ø§Ù„Ù†Ø·Ù‚ Ø¹Ù†Ø¯ Ø§Ø®ØªÙØ§Ø¡ Ø§Ù„ÙŠØ¯ Ø£Ùˆ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø« Ù„ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©
                    elif predicted_class < len(interpreter.actions):
                        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ø§Ø¹Ø±Ø¶ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø«Ù‚Ø©
                        prediction_text = interpreter.actions[predicted_class]
                    
            else:
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙŠØ¯ Ù„ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ©
                if time.time() - last_hand_time > 3.0: # ÙØªØ±Ø© 3 Ø«ÙˆØ§Ù†ÙŠ Ø¨Ø¯ÙˆÙ† ÙŠØ¯ (Ù…Ù† ÙƒÙˆØ¯ test)
                    if interpreter.sentence: # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ø¬Ù…Ù„Ø© ØªÙ… ØªÙƒÙˆÙŠÙ†Ù‡Ø§
                        full_sentence = ' '.join(interpreter.sentence)
                        print(f"Ù†Ø·Ù‚ Ø§Ù„Ø¬Ù…Ù„Ø©: {full_sentence}")
                        speak_elevenlabs(full_sentence) # Ù†Ø·Ù‚ Ø§Ù„Ø¬Ù…Ù„Ø©
                        interpreter.sentence.clear() # Ù…Ø³Ø­ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ø·Ù‚
                    interpreter.reset_after_no_hand() # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙØ³Ø±


            # --- Ø±Ø³Ù… Ø§Ù„ØªØ±Ø§ÙƒØ¨Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± (Ù„ØªØ¸Ù‡Ø± ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…) ---
            # # Ø±Ø³Ù… Ù…Ø³ØªØ·ÙŠÙ„ Ø´ÙØ§Ù ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ù…Ù„Ø© - ØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡ Ù„Ù„Ø¹Ø±Ø¶ ÙÙŠ HTML
            # overlay = image_processed.copy()
            # cv2.rectangle(overlay, (0, 400), (640, 480), (0, 0, 0), -1) # Ø£Ø³ÙˆØ¯
            # alpha = 0.6 # Ø´ÙØ§ÙÙŠØ©
            # image_processed = cv2.addWeighted(overlay, alpha, image_processed, 1 - alpha, 0)

            # # Ø±Ø³Ù… Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù…Ø© - ØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡ Ù„Ù„Ø¹Ø±Ø¶ ÙÙŠ HTML
            # sentence_display = interpreter.get_current_sentence()
            # if sentence_display:
            #     image_processed = draw_arabic_text(image_processed, sentence_display, (10, 420), 36)

            # # Ø±Ø³Ù… Ø´Ø±ÙŠØ· Ø§Ù„Ø«Ù‚Ø© - ØªÙ… Ø¥Ø®ÙØ§Ø¤Ù‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            # if confidence > 0.1: # ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø­Ø¯ Ù…Ø¹ÙŠÙ† Ù„Ù„Ø¹Ø±Ø¶
            #     draw_confidence_bar(image_processed, confidence)

            # # Ø±Ø³Ù… Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© - ØªÙ… Ø¥Ø®ÙØ§Ø¤Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            # draw_styled_landmarks(image_processed, results)

            # Ø§Ù„Ù…Ø¤Ø«Ø± Ø§Ù„Ø¨ØµØ±ÙŠ Ø¹Ù†Ø¯ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©
            if interpreter.visual_flash > 0:
                cv2.rectangle(image_processed, (0, 0), (640, 480), (0, 255, 255), thickness=15) # Ø­Ø¯ÙˆØ¯ ØµÙØ±Ø§Ø¡
                interpreter.visual_flash -= 1

            # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¥Ø·Ø§Ø± Ø¥Ù„Ù‰ JPEG Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¹Ø¨Ø± Ø§Ù„ÙˆÙŠØ¨
            ret, buffer = cv2.imencode('.jpg', image_processed, [int(cv2.IMWRITE_JPEG_QUALITY), 80]) # Ø¬ÙˆØ¯Ø© 80%
            frame_data = base64.b64encode(buffer).decode('utf-8')

            # Ø¥Ø±Ø³Ø§Ù„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ø¥Ø·Ø§Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…ØªØµÙØ­
            socketio.emit('video_frame', { # Ø­Ø¯Ø« Ø¬Ø¯ÙŠØ¯ Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¥Ø·Ø§Ø±
                'hand_detected': hand_detected,
                'prediction': prediction_text, # Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ø³ØªØ¸Ù‡Ø± ÙÙŠ Ø£Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†)
                'confidence': float(confidence),
                'sentence': interpreter.get_current_sentence(), # Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ØªØ±Ø§ÙƒÙ…Ø© (Ø³ØªØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„)
                'new_word': new_word,
                'frame': frame_data
            })
            socketio.sleep(0.01) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø¥ØºØ±Ø§Ù‚ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ù„Ù€ CPU

        # Ø¹Ù†Ø¯ ØªÙˆÙ‚Ù Ø§Ù„Ø­Ù„Ù‚Ø©ØŒ ØªØ­Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        camera_cap.release()
        print("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙˆØªØ­Ø±ÙŠØ±Ù‡Ø§.")
        socketio.emit('camera_stopped', {'message': 'ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.'})

# --- Ù…Ø³Ø§Ø±Ø§Øª Flask ---
@app.route('/')
def index():
    return render_template('index.html')

# Ù…Ø³Ø§Ø± Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© (Ù…Ø«Ù„ Ø§Ù„ØµÙˆØ± ÙˆÙ†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù€ GLB)
@app.route('/static/<path:filename>')
def static_files(filename):
    return app.send_static_file(filename)

# --- Ø£Ø­Ø¯Ø§Ø« SocketIO ---
@socketio.on('connect')
def handle_connect():
    print('Ø¹Ù…ÙŠÙ„ Ù…ØªØµÙ„')
    emit('status', {'message': 'Ù…ØªØµÙ„ Ø¨Ø§Ù„Ø®Ø§Ø¯Ù…'})

@socketio.on('disconnect')
def handle_disconnect():
    print('Ø§Ù†Ù‚Ø·Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„')
    global camera_running, camera_cap
    if camera_running:
        camera_running = False # Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ thread Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¨Ø§Ù„ØªÙˆÙ‚Ù
        # Ø§Ù„Ø«Ø±ÙŠØ¯ Ù†ÙØ³Ù‡ Ø³ÙŠØ­Ø±Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        print("Ø¬Ø§Ø±ÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¨Ø³Ø¨Ø¨ Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„.")

@socketio.on('initialize_model')
def handle_initialize_model(data):
    global interpreter
    
    try:
        model_path = data.get('model_path', 'test') # Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ 'test'
        print(f"Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_path}")
        
        interpreter = SignLanguageInterpreter(model_path)
        
        emit('model_initialized', {
            'status': 'success',
            'message': 'ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­',
            'actions': interpreter.actions
        })
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        error_message = f'Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}'
        print(error_message)
        emit('model_initialized', {
            'status': 'error',
            'message': error_message
        })

@socketio.on('start_camera_stream')
def handle_start_camera_stream():
    global camera_running, camera_thread, interpreter
    if not interpreter:
        emit('camera_error', {'message': 'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ù‡ÙŠØ£. ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.'})
        return

    if not camera_running:
        print("Ø¨Ø¯Ø¡ Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§...")
        camera_running = True
        # Ø¨Ø¯Ø¡ Ø«Ø±ÙŠØ¯ Ø¬Ø¯ÙŠØ¯ Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        camera_thread = threading.Thread(target=camera_stream_thread)
        camera_thread.daemon = True # ÙŠØ³Ù…Ø­ Ù„Ù„Ø®Ø§Ø¯Ù… Ø¨Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø§Ù„Ø«Ø±ÙŠØ¯ Ù‚ÙŠØ¯ Ø§Ù„ØªØ´ØºÙŠÙ„
        camera_thread.start()
        emit('camera_stream_started', {'status': 'success', 'message': 'ØªÙ… Ø¨Ø¯Ø¡ Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.'})
    else:
        emit('camera_stream_started', {'status': 'info', 'message': 'Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„.'})

@socketio.on('stop_camera_stream')
def handle_stop_camera_stream():
    global camera_running, interpreter
    if camera_running:
        print("Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§...")
        camera_running = False # Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ thread Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¨Ø§Ù„ØªÙˆÙ‚Ù
        # Ø§Ù„Ø«Ø±ÙŠØ¯ Ù†ÙØ³Ù‡ Ø³ÙŠØ­Ø±Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        emit('camera_stream_stopped', {'status': 'success', 'message': 'Ø¬Ø§Ø±ÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.'})
        if interpreter:
            interpreter.reset_after_no_hand() # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙØ³Ø± Ø¹Ù†Ø¯ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù
            if interpreter.sentence: # Ù†Ø·Ù‚ Ø£ÙŠ Ø¬Ù…Ù„Ø© Ù…ØªØ¨Ù‚ÙŠØ©
                full_sentence = ' '.join(interpreter.sentence)
                print(f"Ù†Ø·Ù‚ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù: {full_sentence}")
                speak_elevenlabs(full_sentence)
                interpreter.sentence.clear()
    else:
        emit('camera_stream_stopped', {'status': 'info', 'message': 'Ø¨Ø« Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù„ÙŠØ³ Ù†Ø´Ø·Ø§Ù‹.'})

@socketio.on('clear_sentence')
def handle_clear_sentence():
    global interpreter
    if interpreter:
        interpreter.clear_sentence()
    emit('sentence_cleared', {'status': 'success', 'sentence': ''}) # Ø£Ø±Ø³Ù„ Ø¬Ù…Ù„Ø© ÙØ§Ø±ØºØ© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
if __name__ == '__main__':
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    for folder in ['templates', 'static']:
        if not os.path.exists(folder):
            os.makedirs(folder)
            print(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯: {folder}")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    required_files = ['test.h5', 'test_actions.txt', 'test_scaler_mean.npy', 'test_scaler_scale.npy']
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
            
    font_path_check = os.path.join(os.path.dirname(__file__), "NotoNaskhArabic-Regular.ttf")
    if not os.path.exists(font_path_check):
        print("âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ 'NotoNaskhArabic-Regular.ttf' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù†ÙØ³ Ù…Ø¬Ù„Ø¯ 'app.py'. Ù‚Ø¯ Ù„Ø§ ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.")
        print("  ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ù†: https://fonts.google.com/noto/specimen/Noto+Naskh+Arabic")
        print("  Ø«Ù… Ø¶Ø¹ Ø§Ù„Ù…Ù„Ù NotoNaskhArabic-Regular.ttf ÙÙŠ Ù†ÙØ³ Ù…Ø¬Ù„Ø¯ app.py")

    if missing_files:
        print("âš ï¸  Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙÙ‚ÙˆØ¯Ø©:")
        for file in missing_files:
            print(f"   - {file}")
        print("\nØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹.")
    else:
        print("âœ… Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©.")
    
    print("\nğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…...")
    print(f"ğŸŒ Ø³ÙŠÙƒÙˆÙ† Ø§Ù„Ø®Ø§Ø¯Ù… Ù…ØªØ§Ø­Ø§Ù‹ Ø¹Ù„Ù‰: http://localhost:5001 (Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {CAMERA_INDEX})")
    print("ğŸ“± Ø£Ùˆ Ø¹Ù„Ù‰ Ø´Ø¨ÙƒØªÙƒ Ø§Ù„Ù…Ø­Ù„ÙŠØ©: http://YOUR_IP:5001")
    
    try:
        # ØªØ´ØºÙŠÙ„ SocketIO app
        socketio.run(app, host='0.0.0.0', port=5001, debug=True, allow_unsafe_werkzeug=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¯ÙˆÙŠÙ‹Ø§.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…: {e}")
